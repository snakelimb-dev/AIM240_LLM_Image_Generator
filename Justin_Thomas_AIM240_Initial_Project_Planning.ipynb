{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPFHap1832Jd"
   },
   "source": [
    "# AIM 240: AI/ML Capstone Project Initialization Document\n",
    "\n",
    "**Student Name:**\n",
    "\n",
    "**Semester/Year:** Spring 2026\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsb377tA32Jf"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to your Capstone Project! This notebook will guide you through the process of defining, scoping, and planning your individual AI/ML project. Since this course is asynchronous, this document serves as our initial communication tool for understanding your vision and providing targeted feedback.\n",
    "\n",
    "**Please complete ALL sections thoroughly.** Incomplete submissions will be returned for revision before approval.\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "1. Read through the entire notebook first to understand what's expected\n",
    "2. Complete Section 1 (Project Ideation) with 2-3 potential project ideas\n",
    "3. Use Section 2 to systematically evaluate and rank your ideas\n",
    "4. Complete Sections 3-12 for your **top-ranked project only**\n",
    "5. Export as HTML or PDF and submit via the course LMS by the posted deadline\n",
    "6. Await instructor feedback before beginning implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vTVXC4L32Jg"
   },
   "source": [
    "---\n",
    "\n",
    "## Section 1: Project Ideation\n",
    "\n",
    "### 1.1 Brainstorming Your Project Ideas\n",
    "\n",
    "Before committing to a single project, explore multiple possibilities. List **2-3 distinct project ideas** that interest you. For each idea, provide a brief description (3-5 sentences) covering:\n",
    "\n",
    "- What problem does it solve?\n",
    "- Who would benefit from this solution?\n",
    "- What type of AI/ML would be involved (classification, generation, detection, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2jsTISglN3f"
   },
   "source": [
    "## Project Idea A: Using Gambeoy memory Recordings to make a Neural model .\n",
    "\n",
    "I have completed a frame recorder which can run the pyboy emulator and record The ROM page WRAMand VRAM data along side the player button input.\n",
    "\n",
    "I made code that will randomly press the buttons and make a dataset of what ever desired size. Right now I am using 75 recordings, 2000 frames long.\n",
    "\n",
    "My hope is that by pretraining the model on an input reconstruction task, I can then train a dynamics model on top that will predict the delta or changes in memory. \n",
    "\n",
    "I have pretrained a hi fidelity 8x8 2bpp tile encoder that I use to generate embeddings for ROM, WRAM and VRAM patches. \n",
    "\n",
    "I am experimenting with slot attention becuase it sounds like the sort of structure I want, where the network is tracking objects.\n",
    "\n",
    "I chosen 64 slots, of dimension 128. This is twice the tile embedding and a little larger than the actual number of objects in gameboy memory, which is only 40. My reasoning is that the background layer also contains object like tiles.\n",
    "\n",
    "Primary domains: [multimodal, Transformer, generative AI]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlwoRPxp32Jg"
   },
   "source": [
    "#### IDEA B: LLM Projection to Image generator.\n",
    "\n",
    "**Brief Description:**\n",
    "\n",
    "<span style=\"color:blue\">[Finetune  or place a head on a model like Qwen Vl 8B  and have it output embedding tokens for an image generator, either my own pixel art diffusion model I have already created or something like stable diffusion. Maybe learn some tricks from Meta's Chameleon model. From discussions with gemini, research examples have done this using a frozen llm, so that sounds appealing. An exciting possibility, if I could get the image output generated fast enough, could be the start of a game-like interface. A related avenue is Experimenting with feed back loops where the model outputs an image then modifies it in a painterly way. Doing this project as a clone of Instruct Pix2pix would also be great. I am referring to a text conditioned diffusionmodel that can be given an input image and text editing instructions and it will edit a portion of the input image. I would be willing to construct and even larger dataset, but perhap with more video game themed edits, or things lke player conroller input, like the d-pad. I would be willing to drop the resolution of the model to something more inline with classic video games and not more than 512x512. lastly exploring preference tuning on pix2pix or image generating llms would be interesting. I thought since companies will have model perform something like RLHF. what if I did an RLHF with one of these LLM image generators. Have a VLM judge output and express preference between image outputs. This would not be on a frozen LLM but there are many finetuning frameworks, and also preference tuning, right? ]</span>\n",
    "\n",
    "**Primary ML Domain(s):** <span style=\"color:blue\">[ Computer Vision, NLP, Generative AI, Multi-modal]</span>\n",
    "\n",
    "gpt suggested data sources:\n",
    "| Dataset                     | Size    | Caption Quality | Best For                    |\n",
    "| --------------------------- | ------- | --------------- | --------------------------- |\n",
    "| **MS-COCO**                 | 330k    | ⭐⭐⭐⭐⭐ (human)   | Grounded objects, actions   |\n",
    "| **Flickr30k**               | 31k     | ⭐⭐⭐⭐⭐           | Fast prototyping            |\n",
    "| **Conceptual Captions 3M**  | 3M      | ⭐⭐⭐⭐            | Medium-scale training       |\n",
    "| **Conceptual Captions 12M** | 12M     | ⭐⭐⭐             | Larger noisy training       |\n",
    "| **LAION-400M / 5B**         | 400M–5B | ⭐⭐              | Full-scale diffusion models |\n",
    "| **WIT (Wikipedia)**         | 37M     | ⭐⭐⭐⭐            | Factual / entity grounding  |\n",
    "| **WikiArt**                 | 80k     | ⭐⭐⭐ (labels)    | Artistic style learning     |\n",
    "| **Synthetic (I am comfortable with the concept of data augmentation)**  | Any     | ⭐⭐⭐⭐⭐           | Controllability, structure  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGbXZS9alRog"
   },
   "source": [
    "\n",
    "\n",
    "Project Idea C\n",
    "Working Title: [GODOT or Pygame specialized dataset- Vetted code]\n",
    "\n",
    "Brief Description:\n",
    "\n",
    "[Create a dataset of Godot Code using local models and commercial models in a mixture of code vetting. Would benefit learing from a curriculum which I would have to establish. I might be able to use for example the alpaca clean dataset and a english to french dataset to spice up the generation, and ask it to make a coding example based on say every method available for each node in the Godot Game engine. I could pick particular design patterns from the gang of four book, and implement those systems in Godot. Perhaps use a Rag system to retrieve other working samples so it can perform trial and error, but then maybe in the future when it uses that function it will see a proper example and use that instead. Here we consider a training lookp where the current llm generating the data only benefits from prior experience in the form of  a rag interface, presenting relevent examples. Eventually the whole corpus could then be used to fine tune an llm. ehh... but maybe it should be alway a RAG system. Shrugs. This is what benchmarks are for , right? While commercial model know some GD script , local ones certainingly need some help. It should be possible to make them do better because I feel GDscript is under represented but I can't really prove this yet.]\n",
    "\n",
    "Primary ML Domain(s): [ NLP,  Other]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9vTruX232Jh"
   },
   "source": [
    "#### Project Idea D (Optional but Recommended)\n",
    "\n",
    "**Working Title:** <span style=\"color:blue\">[Synth speaking and/or Singing Moshi Full Dulpex Voice Assistant]</span>\n",
    "\n",
    "**Brief Description:**\n",
    "\n",
    "<span style=\"color:blue\">[It could help people wanting to interact with LLMs in a different way, and because of the duplex thing, it might be able to interact with music in a harmonic and perhaps even rhythmically aware way. Explore using embellishment technique markup to denote special vocal articulations. something like this list:\n",
    "https://imgv2-2-f.scribdassets.com/img/document/380229964/original/7aeedd745b/1588863549?v=1\n",
    " Explore audio generation which was removed from ChatGpt's advanced voice mode.]</span>\n",
    "\n",
    "**Primary ML Domain(s):** <span style=\"color:blue\">[Select all that apply: Computer Vision, NLP, Tabular/Structured Data, Time Series/Forecasting, Recommender Systems, Reinforcement Learning, Generative AI, Multi-modal, Other]\\</span>\n",
    "\n",
    "### additional notes:\n",
    "\n",
    "Make a dataset to finetune the Moshi model, get it working on my local machine, or serve it up online. Make a dataset that include pitch and scale annotations.\n",
    "\n",
    "Since it models and response to a user audio stream in a duplex manner, I have hopes maybe it could recognize a scale and try to play something back.\n",
    "\n",
    "Make samples of monophinic synths moving up and down scales, perhaps using different timbres. In order to finetune I would need to use rented compute, which would be a good lesson.\n",
    "\n",
    "The dataset I found was not terribly large. I could perhaps perform data augmentation on these samples by transposing the samples and adjustin g the labels accordingly. I could maybe find differnt datasets, but multiple singers might present an issue.\n",
    "\n",
    "https://zenodo.org/records/1442513?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4RFPjhO32Ji"
   },
   "source": [
    "---\n",
    "\n",
    "## Section 2: Project Ranking & Selection\n",
    "\n",
    "### 2.1 Evaluation Criteria Matrix\n",
    "\n",
    "Rate each of your project ideas on a scale of **1-5** for each criterion below. Be honest in your assessment - this exercise helps you avoid projects that may become problematic later.\n",
    "\n",
    "**Rating Scale:**\n",
    "- 1 = Very Poor / Major Concerns\n",
    "- 2 = Below Average / Some Concerns\n",
    "- 3 = Adequate / Neutral\n",
    "- 4 = Good / Minor Concerns\n",
    "- 5 = Excellent / No Concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Js3G1XZ132Jj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "PROJECT EVALUATION MATRIX\n",
      "===========================================================================\n",
      "      Criterion         Idea A  Idea B  Idea C  Idea D\n",
      "     Personal Interest     5       4       5       4  \n",
      "     Data Availability     5       2       4       3  \n",
      " Technical Feasibility     3       2       4       3  \n",
      " Scope Appropriateness     4       4       4       3  \n",
      "Novelty/Learning Value     5       4       4       4  \n",
      "       Portfolio Value     4       3       4       4  \n",
      "       Ethical Clarity     2       3       4       5  \n",
      "   MLOps Applicability     4       3       5       3  \n",
      "           TOTAL SCORE    32      25      34      29  \n",
      "\n",
      "===========================================================================\n",
      "Maximum possible score: 40\n",
      "===========================================================================\n",
      "Top Recommendation: Idea C (34 points)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# PROJECT RANKING CALCULATOR\n",
    "# Fill in your scores below (1-5 for each criterion)\n",
    "# ============================================================\n",
    "\n",
    "# Define the evaluation criteria\n",
    "criteria = [\n",
    "    \"Personal Interest\",      # How excited are you about this project?\n",
    "    \"Data Availability\",      # Is appropriate data publicly available?\n",
    "    \"Technical Feasibility\",  # Can this be accomplished with your current skills + learning?\n",
    "    \"Scope Appropriateness\",  # Is this achievable in one semester?\n",
    "    \"Novelty/Learning Value\", # Will you learn new skills beyond what you know?\n",
    "    \"Portfolio Value\",        # Will this impress future employers?\n",
    "    \"Ethical Clarity\",        # Are there minimal ethical concerns?\n",
    "    \"MLOps Applicability\"     # Can you apply MLOps concepts?\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# ENTER YOUR SCORES HERE (1-5 for each)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "idea_a_scores = {\n",
    "    \"Personal Interest\":      5,    # Score 1-5\n",
    "    \"Data Availability\":      5,    # Score 1-5\n",
    "    \"Technical Feasibility\":  3,    # Score 1-5\n",
    "    \"Scope Appropriateness\":  4,    # Score 1-5\n",
    "    \"Novelty/Learning Value\": 5,    # Score 1-5\n",
    "    \"Portfolio Value\":        4,    # Score 1-5\n",
    "    \"Ethical Clarity\":        2,    # Score 1-5\n",
    "    \"MLOps Applicability\":    4     # Score 1-5\n",
    "}\n",
    "\n",
    "idea_b_scores = {\n",
    "    \"Personal Interest\":      4, \n",
    "    \"Data Availability\":      2, \n",
    "    \"Technical Feasibility\":  2, \n",
    "    \"Scope Appropriateness\":  4, \n",
    "    \"Novelty/Learning Value\": 4, \n",
    "    \"Portfolio Value\":        3, \n",
    "    \"Ethical Clarity\":        3, \n",
    "    \"MLOps Applicability\":    3\n",
    "}\n",
    "\n",
    "idea_c_scores = {\n",
    "    \"Personal Interest\":      5, \n",
    "    \"Data Availability\":      4, \n",
    "    \"Technical Feasibility\":  4, \n",
    "    \"Scope Appropriateness\":  4, \n",
    "    \"Novelty/Learning Value\": 4, \n",
    "    \"Portfolio Value\":        4, \n",
    "    \"Ethical Clarity\":        4, \n",
    "    \"MLOps Applicability\":    5\n",
    "}\n",
    "\n",
    "idea_d_scores = {\n",
    "    \"Personal Interest\":      4, \n",
    "    \"Data Availability\":      3, \n",
    "    \"Technical Feasibility\":  3, \n",
    "    \"Scope Appropriateness\":  3, \n",
    "    \"Novelty/Learning Value\": 4, \n",
    "    \"Portfolio Value\":        4, \n",
    "    \"Ethical Clarity\":        5, \n",
    "    \"MLOps Applicability\":    3\n",
    "}\n",
    "# ============================================================\n",
    "# CALCULATION & DISPLAY\n",
    "# ============================================================\n",
    "\n",
    "data = {\n",
    "    \"Criterion\": criteria,\n",
    "    \"Idea A\": [idea_a_scores[c] for c in criteria],\n",
    "    \"Idea B\": [idea_b_scores[c] for c in criteria],\n",
    "    \"Idea C\": [idea_c_scores[c] for c in criteria],\n",
    "    \"Idea D\": [idea_d_scores[c] for c in criteria]\n",
    "}\n",
    "\n",
    "df_scores = pd.DataFrame(data)\n",
    "\n",
    "# Calculate totals\n",
    "totals = pd.DataFrame({\n",
    "    \"Criterion\": [\"TOTAL SCORE\"],\n",
    "    \"Idea A\": [sum(idea_a_scores.values())],\n",
    "    \"Idea B\": [sum(idea_b_scores.values())],\n",
    "    \"Idea C\": [sum(idea_c_scores.values())],\n",
    "    \"Idea D\": [sum(idea_d_scores.values())]\n",
    "})\n",
    "\n",
    "# Combine for display\n",
    "df_display = pd.concat([df_scores, totals], ignore_index=True)\n",
    "\n",
    "print(\"=\" * 75)\n",
    "print(\"PROJECT EVALUATION MATRIX\")\n",
    "print(\"=\" * 75)\n",
    "print(df_display.to_string(index=False, justify='center'))\n",
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(f\"Maximum possible score: {len(criteria) * 5}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "# Suggestion based on scores\n",
    "winner_val = totals.iloc[0, 1:].max()\n",
    "if winner_val > 0:\n",
    "    winner_name = totals.iloc[0, 1:].idxmax()\n",
    "    print(f\"Top Recommendation: {winner_name} ({winner_val} points)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0m4l7T5H32Jk"
   },
   "source": [
    "### 2.2 Final Ranking & Justification\n",
    "\n",
    "**Your Final Ranking:**\n",
    "\n",
    "1. **First Choice:** <span style=\"color:blue\">[Enter project title]</span>\n",
    "2. **Second Choice:** <span style=\"color:blue\">[Enter project title]</span>\n",
    "3. **Third Choice:** <span style=\"color:blue\">[Enter project title or N/A]</span>\n",
    "\n",
    "**Justification for Your Top Choice (3-5 sentences):**\n",
    "\n",
    "<span style=\"color:blue\">[Explain why your first choice is the best fit for you. Consider not just the scores, but any qualitative factors that influenced your decision.]</span>\n",
    "\n",
    "**What would need to change for you to switch to your second choice?**\n",
    "\n",
    "<span style=\"color:blue\">[Your response here - this helps identify risk factors]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Problem Statement\n",
    "\n",
    "### 3.1 Problem Definition\n",
    "**Your Problem Statement:**\n",
    "Current approaches to learning game dynamics and world models typically rely on raw pixel data. This method is computationally expensive and inefficient because the model must implicitly learn to disentangle objects, backgrounds, and game logic from a high-dimensional visual stream. This results in models that struggle to maintain object permanence or understand discrete state changes. By utilizing a \"glass-box\" approach that accesses the Gameboy's internal memory states (WRAM, VRAM, and ROM) directly, this project aims to train a dynamics model that is significantly more data-efficient and interpretable. This model will use an object-centric architecture (Slot Attention) to predict memory state deltas, effectively learning the \"physics\" of the game engine at the instruction level rather than the visual level.\n",
    "\n",
    "### 3.2 Existing Solutions\n",
    "\n",
    "| Solution | Type | Strengths | Weaknesses | Why Yours Will Be Different |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **DreamerV3 / World Models (Ha & Schmidhuber)** | Pixel-based Generative Models (VAE/RNN) | Highly generalizable; works on any game just by looking at the screen. | Computationally expensive; struggles to maintain small, precise details (e.g., a 1-pixel bullet); prone to \"hallucinating\" visual artifacts that don't exist in the game engine. | My solution accesses the \"ground truth\" memory state (VRAM/WRAM) directly. It predicts discrete memory changes rather than hallucinating pixels, ensuring 100% graphical consistency and requiring significantly less compute. |\n",
    "| **OpenAI Gym (RAM Observation Mode)** | Raw Byte-Input Reinforcement Learning | Extremely fast processing; zero visual rendering overhead. | Raw RAM is unstructured. A neural network struggles to understand that Byte A (X-coord) and Byte B (Y-coord) are related to Sprite C. It lacks spatial context. | My project imposes structure on the memory. By using **Tile Encoders** and **Slot Attention**, I explicitly model the *spatial* relationship of memory (how VRAM tiles relate to screen position), bridging the gap between raw bytes and visual understanding. |\n",
    "| **Object-Centric Learning (e.g., SAVi, SLOT-Attention)** | Unsupervised Object Discovery | Good at separating backgrounds from moving objects in video feeds. | Often fragile; requires heavy training to learn \"what is an object\" from pixels alone; frequently merges or loses objects during occlusion. | My model does not need to *guess* what an object is. The Gameboy's Object Attribute Memory (OAM) explicitly defines sprites. I use this architectural fact to initialize slots, making object tracking deterministic rather than probabilistic. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 4: Project Scope Definition\n",
    "\n",
    "### 4.1 Core Features (Must-Have)\n",
    "\n",
    "| # | Feature | Description | Success Looks Like |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **1** | **Automated PyBoy Recorder**<br>✅ *(COMPLETED)* | A robust pipeline that runs the emulator (DMG/GBC), executes agents, and serializes WRAM, VRAM, OAM, and full color frame buffers frame-by-frame across the Gameboy/GBC library. | Dataset of >100k frames with synced inputs and memory dumps, allowing for **pixel-perfect color frame reconstruction** as a ground-truth visual target. |\n",
    "| **2** | **Pretrained 8x8 Tile Encoder**<br>✅ *(COMPLETED)* | A frozen, high-fidelity CNN backbone that compresses raw 8x8 VRAM/ROM tiles into compact latent embeddings. | Achieve **99% reconstruction accuracy** on individual tiles, providing the dynamics model with a perfect \"visual dictionary.\" |\n",
    "| **3** | **Masked Frame Reconstructor** | A global decoding module that reconstructs the full visual state from latent embeddings, specifically designed to handle and \"fill in\" partially masked inputs. | High-fidelity side-by-side reconstruction where the model accurately predicts missing visual/memory context from partial data. |\n",
    "| **4** | **Visual Debugger / Dashboard**<br>✅ *(COMPLETED)* | A real-time visualization suite that renders the model's internal latent predictions back into a viewable Gameboy frame for verification. | A functioning \"World Model\" viewer showing ground truth vs. model hallucination in a side-by-side format. |\n",
    "| **5** | **Latent Dynamics Model** | The core neural network (Transformer/Slot-Attention) that predicts the next memory state embedding based on current state + user input. | Consistent decrease in MSE loss and convergence on stable state-to-state transitions across multiple game titles. |\n",
    "\n",
    "\n",
    "### 4.2 Extended Features (Nice-to-Have)\n",
    "\n",
    "| Priority | Feature | Description | Dependencies |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **1** | **Real-Time Inference** | Optimization of the model to process current memory states and predict the next state within a timeframe usable for live interaction (targeting 60 FPS). | Core Feature 4. |\n",
    "| **2** | **Long-Horizon \"Dreaming\"** | Allowing the model to hallucinate 50-100 frames into the future without any ground-truth data. | Stable Dynamics Model. |\n",
    "| **3** | **Model-Based RL Agent** | Training an agent that uses the learned world model to plan moves rather than the emulator. | Long-horizon stability. |\n",
    "| **4** | **Transfer Learning / Zero-Shot** | Testing the model's ability to predict dynamics on a Gameboy or GBC title it was not specifically trained on. | Core Features 2 & 4. |\n",
    "| **5** | **Text Interaction** | A natural language interface enabling users to prompt the model or \"talk\" to the game state (e.g., \"Go to the next level\"). | Stable Dynamics Model. |\n",
    "| **6** | **ROM Generation** | Attempting to compile the learned model weights or logic back into a playable Gameboy binary file. | High-fidelity model accuracy. |\n",
    "\n",
    "### 4.3 Out of Scope\n",
    "**This project will NOT include:**\n",
    "* **Sound / Audio Processing:** The APU and sound generation are excluded.\n",
    "* **Multiplayer:** Link cable emulation or multi-agent support.\n",
    "* **Loading State Back to Emulator:** Writing the neural network's predicted state back into the live PyBoy emulator memory for gameplay (read-only visualization is the limit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 5: Data Sources, Collection, and Validation\n",
    "\n",
    "### 5.2 Data Sources\n",
    "\n",
    "**Data Source 1**\n",
    "* **Name/URL:** PyBoy Emulator Internal State Recordings (Self-Generated)\n",
    "* **Type:** Synthetic / Self-collected via Simulation\n",
    "* **Access Method:** Generated locally using a custom Python script wrapping the PyBoy emulator. The script records WRAM (Work RAM), VRAM (Video RAM), and ROM page data alongside player button inputs for every frame.\n",
    "* **License/Terms:** MIT License (PyBoy); Game ROMs can be procedurally generated through example code I have found. I am also using ROMs from games I own.\n",
    "* **Quality Assessment:** High quality and noiseless. Since the data is captured directly from the emulator's memory bus, it is deterministic and free from visual artifacts (blur, compression) found in video-based datasets.\n",
    "* **Volume Available:** Currently 150,000 frames (75 recordings × 2,000 frames). Scalable to infinite volume as it is procedurally generated.\n",
    "\n",
    "**Data Source 2**\n",
    "* **Name/URL:** Pre-trained 8x8 Tile Encoders\n",
    "* **Type:** Self-collected / Derived Feature Set\n",
    "* **Access Method:** Generated by processing the raw VRAM tile maps through a custom pre-trained encoder.\n",
    "* **License/Terms:** N/A (Internal project artifact).\n",
    "* **Quality Assessment:** Validated 2bpp (2-bit per pixel) fidelity. The encoder has been verified to accurately represent the 8x8 pixel patches used by the Gameboy hardware.\n",
    "* **Volume Available:** Continuous coverage for all generated frames in Data Source 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5.3 Data Collection Plan\n",
    "\n",
    "**Data Collection Methodology:**\n",
    "I have built a custom frame recorder that interfaces with the PyBoy emulator. The collection pipeline operates as follows:\n",
    "1.  **Agent Policy:** A random-agent script generates inputs (button presses) to navigate the game environment. Start button presses are more frequent early on.\n",
    "2.  **Capture Cycle:** For every frame rendered, the script freezes the emulator state and dumps the raw byte arrays for WRAM, VRAM, and the active ROM bank.\n",
    "3.  **Storage:** Data is serialized into a structured format (likely `.npy` or `.pt`) pairing the memory state with the specific button input that caused the transition.\n",
    "4.  **Timeline:** The initial dataset of 150,000 frames is complete. I plan to generate a larger validation set (approx. 50,000 frames) with a different random seed to test generalization.\n",
    "5.  **Assumption:** I assume that data from the beginning of many games will be sufficient, because it is much harder to get save points depper into the game. So the model will not be seeing as much data as it could.\n",
    "6.  **Start Point recorder** So that an random agent can begin playing immediately, I made a interface that makes it easy to record the first playable moment for each game. This require a human to record these srt points. I decided to try to see how well it performs just using the beginnings of games, then I can add in data later that goes \"deeper\" into the ROM.\n",
    "\n",
    "**Data Augmentation Strategy (if applicable):**\n",
    "Standard visual augmentations (flipping/rotation) are **not** applicable here because they would corrupt the memory-address logic (e.g., flipping a sprite in VRAM changes its binary definition, not just its appearance).\n",
    "Instead, augmentation will focus on:\n",
    "* **State Mixing:** Randomizing the starting state of the emulator (using save states) to ensure the model sees a variety of game levels and scenarios, not just the opening sequence.\n",
    "* **Input Noise:** Occasional \"sticky keys\" or \"no-op\" sequences to simulate more varied player behavior than pure random noise.\n",
    "\n",
    "**Reruns:** I can create new random button presses for any game, essentially making new augmentations.\n",
    "\n",
    "### 5.4 Data Validation Checklist\n",
    "[x] I have verified that my primary data source is accessible right now\n",
    "[x] I have reviewed the license and confirmed I can use this data for my project\n",
    "[x] I have examined sample data and understand its structure\n",
    "[x] I have a backup plan if my primary data source becomes unavailable\n",
    "[x] My data volume is sufficient for training a meaningful model\n",
    "[x] I have considered and documented any data quality issues\n",
    "\n",
    "**Backup Data Plan:**\n",
    "Since the primary data source is a local generator I control, the risk of \"unavailability\" is low. However, if the random-agent approach fails to explore enough of the game states (e.g., getting stuck in menus), my backup plan is to record manual gameplay footage (playing the game myself) and feed those input logs into the recorder to generate high-quality, human-level state traversals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Technical Approach\n",
    "\n",
    "### 6.1 High-Level Architecture\n",
    "\n",
    "The system implements a multi-modal generative architecture designed to reverse-engineer and simulate the internal state of a Game Boy Color. It treats the console's state not just as a grid of pixels, but as a fusion of visual data (VRAM) and system memory (WRAM/ROM), using an object-centric **Slot Attention** mechanism to bind these modalities together.\n",
    "\n",
    "1. **Input:**\n",
    "    * **Raw State Data:** JSON-based recordings of Game Boy execution, containing frame-by-frame snapshots.\n",
    "    * **Visual Data:** Raw VRAM banks (0 and 1) parsed into 8x8 pixel tiles and Background Map indices.\n",
    "    * **Memory Data:** Raw byte arrays for WRAM (Working RAM) and ROM (Read-Only Memory) banks.\n",
    "\n",
    "2. **Processing:**\n",
    "    * **Visual Encoding:** VRAM tiles are passed through a frozen, pre-trained Convolutional Neural Network (CNN) to extract 64-dimensional latent embeddings for each unique tile.\n",
    "    * **Memory Patching:** 16KB memory banks are \"patchified\" into 64-byte chunks, normalized to floating-point values, and embedded with positional information.\n",
    "    * **Masking:** A portion of the visual tiles and memory patches are randomly masked during training to force the model to learn structural dependencies (Masked Auto-Encoder approach).\n",
    "\n",
    "3. **Model:**\n",
    "    * **Visual Cortex:** A pre-trained Visual Autoencoder (CNN) that handles tile compression and reconstruction.\n",
    "    * **Memory Cortex:** Three distinct Transformer Encoders that process WRAM and ROM as sequential \"language\" data.\n",
    "    * **Context Fusion:** A spatial encoder that combines the screen layout (Background Map) with memory embeddings.\n",
    "    * **Slot Attention:** The core bottleneck layer that iteratively binds visual and memory inputs into **N** discrete \"slots\" representing objects or game entities.\n",
    "\n",
    "4. **Output:**\n",
    "    * **Reconstructed Screen:** A full 160x144 pixel frame generated by decoding the slot representations back into pixel space.\n",
    "    * **Reconstructed Memory:** Predicted byte values for WRAM and ROM, effectively hallucinating the system state that corresponds to the visual output.\n",
    "\n",
    "5. **Interface:**\n",
    "    * **Training CLI:** A Python-based command-line interface for managing data streaming and training loops.\n",
    "    * **Visualization Dashboard:** A real-time Matplotlib dashboard that renders the Ground Truth vs. Reconstructed screens side-by-side with heatmaps of the memory banks.\n",
    "\n",
    "---\n",
    "\n",
    "**Architecture Flow:**\n",
    "\n",
    "* **Inputs:** VRAM Tiles, WRAM/ROM Bytes, and Background Maps flow into the system.\n",
    "* **Encoders:** CNNs extract visual features while Transformers process memory sequences.\n",
    "* **Bottleneck:** A Fusion Layer combines all data into a Context-Aware Slot Attention module, which distills the frame into discrete object \"slots.\"\n",
    "* **Decoders:** The slots are projected back into 64-dimensional latents for the CNN Pixel Decoder and raw byte predictions for the Memory Projectors.\n",
    "* **Final Output:** A reconstructed 160x144 screen and a heatmap of predicted memory states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Model Selection\n",
    "\n",
    "**Primary Model Architecture:**\n",
    "**Multi-Modal Slot-Attention Transformer.** This architecture is chosen because the Game Boy’s state is fundamentally \"object-oriented\" (sprites and background tiles). Slot Attention allows the model to learn a discrete, object-centric bottleneck that maps shared visual features to specific memory addresses. The Transformer backbone handles the long-range dependencies required to understand how a byte changed in WRAM corresponds to a pixel change in VRAM across the frame.\n",
    "\n",
    "**Baseline Model:**\n",
    "**Mean-Tile Linear Reconstructor.** A simple linear layer that attempts to predict tile latents directly from a flattened vector of all memory banks. This establishes a \"minimum performance\" baseline by showing how much a non-spatial, non-attentive model can reconstruct using raw memory correlation alone.\n",
    "\n",
    "**Alternative Models to Explore:**\n",
    "\n",
    "| Model | Why Consider It | When to Try It |\n",
    "| :--- | :--- | :--- |\n",
    "| **VQ-VAE** | To discretize the latent space into a fixed \"tile vocabulary.\" | If the CNN latent space remains too \"blurry\" or continuous for sharp reconstructions. |\n",
    "| **Temporal Transformer (GPT-style)** | To predict the *next* frame based on current memory and inputs. | Once static frame reconstruction reaches >90% accuracy. |\n",
    "| **U-Net Decoder** | To use skip-connections for spatial refinement. | If Slot Attention fails to capture fine-grained background details. |\n",
    "\n",
    "**Example Model Selection (Game Boy Project):**\n",
    "* **Primary:** Context-Aware Slot Attention (Iterative refinement of object latents).\n",
    "* **Baseline:** MLP (Multilayer Perceptron) mapping WRAM to VRAM latents.\n",
    "* **Alternatives:** Vision Transformer (ViT) if the spatial complexity of the background exceeds CNN capabilities. or Vit-Mae."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.5 GBC State Reconstruction Model Diagram\n",
    "\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph Input_Processing\n",
    "    V[Visual Tiles] --> CNN[Frozen Encoder]\n",
    "    M[WRAM/ROM] --> MT[Mem Transformer]\n",
    "    end\n",
    "\n",
    "    subgraph Attention_Mechanism\n",
    "    CNN --> Proj[Feature Projection]\n",
    "    MT --> Global[Global Context]\n",
    "    Proj --> Slot[Slot Attention]\n",
    "    Global --> Slot\n",
    "    end\n",
    "\n",
    "    subgraph Reconstruction\n",
    "    Slot --> TileDec[Tile Decoder]\n",
    "    Slot --> MemDec[Mem Decoder]\n",
    "    end\n",
    "\n",
    "    TileDec --> OutV[Reconstructed VRAM]\n",
    "    MemDec --> OutM[Reconstructed WRAM]\n",
    "\n",
    "    style CNN fill:#f9f,stroke:#333\n",
    "    style Slot fill:#69f,stroke:#333,stroke-width:4px\n",
    "    style MT fill:#dfd,stroke:#333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.6 Full World Model Diagram\n",
    "\n",
    "The architecture follows a \"Reconstruct-then-Predict\" pipeline. The static model first distills the current frame into object-centric slots; the Temporal Dynamics model then consumes these slots to predict changes in both the visual environment and the internal system memory.\n",
    "\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph Input_Processing [Static Input Processing]\n",
    "    V[VRAM Tiles_t] --> CNN[Frozen CNN Encoder]\n",
    "    M[WRAM/ROM_t] --> MT[Mem Transformer]\n",
    "    end\n",
    "\n",
    "    subgraph Attention_Mechanism [State Distillation]\n",
    "    CNN --> Proj[Feature Projection]\n",
    "    MT --> Global[Global Context]\n",
    "    Proj --> Slot[Slot Attention]\n",
    "    Global --> Slot\n",
    "    end\n",
    "\n",
    "    subgraph Reconstruction [Static Decoding]\n",
    "    Slot --> TileDec[Tile Decoder]\n",
    "    Slot --> MemDec[Mem Decoder]\n",
    "    TileDec --> OutV[Reconstructed VRAM_t]\n",
    "    MemDec --> OutM[Reconstructed WRAM_t]\n",
    "    end\n",
    "\n",
    "    subgraph Temporal_Dynamics [Sequential Prediction]\n",
    "    Slot --> T_Trans[Temporal Transformer]\n",
    "    Input[User Input/Action] --> T_Trans\n",
    "    T_Trans --> DeltaV[VRAM Delta Prediction]\n",
    "    T_Trans --> DeltaM[WRAM Delta Prediction]\n",
    "    end\n",
    "\n",
    "    DeltaV --> NextV[Predicted VRAM_t+1]\n",
    "    DeltaM --> NextM[Predicted WRAM_t+1]\n",
    "\n",
    "    style CNN fill:#f9f,stroke:#333\n",
    "    style Slot fill:#69f,stroke:#333,stroke-width:4px\n",
    "    style MT fill:#dfd,stroke:#333\n",
    "    style T_Trans fill:#fb2,stroke:#333,stroke-width:2px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Training Strategy\n",
    "\n",
    "**Training Approach:** **Multi-Stage Pipeline (Pre-training + Modular Integration + Future Temporal Stacking)**\n",
    "\n",
    "**Justification:**\n",
    "The complexity of mapping raw memory bytes to visual pixels necessitates a staged approach to ensure each component handles its specific domain before they are combined:\n",
    "1. **Data Collection & Extraction:** Gathering raw execution snapshots and decomposing VRAM into discrete 8x8 tiles to build a representative visual library.\n",
    "2. **Pre-training (Visual Cortex):** Training a Convolutional Autoencoder (CNN) on individual tiles from the dataset. This ensures the model can compress and reconstruct the \"atoms\" of the Game Boy display before attempting to understand their relationship to memory.\n",
    "3. **Full Model Training (Reconstruction):** Integrating the frozen CNN with Memory Transformers and Slot Attention. The model is trained to reconstruct the full 160x144 frame from partially masked inputs, forcing it to learn the causal link between CPU state (RAM) and display output.\n",
    "4. **Temporal Expansion:** Once static reconstruction is high-fidelity, a temporal dynamics model will be placed on top to predict state transitions ($S_t \\rightarrow S_{t+1}$), enabling the system to \"play\" or simulate the game autonomously.\n",
    "\n",
    "**Hyperparameter Tuning Plan:**\n",
    "We will utilize **Random Search** followed by **Manual Refinement** on the following key areas:\n",
    "* **Slot Count:** Testing the bottleneck capacity (e.g., 32, 64, or 128 slots) to find the point where object separation is cleanest without losing background detail.\n",
    "* **Masking Ratios:** Adjusting the percentage of VRAM and WRAM masked (15% to 50%) to optimize the model’s \"hallucination\" capabilities.\n",
    "* **Learning Rate Schedule:** Implementing a OneCycleLR policy to stabilize the training of the Transformers and Slot Attention module simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HorFQSSZ32Jn"
   },
   "source": [
    "### 6.4 Technology Stack\n",
    "\n",
    "The stack is built on a modular Python ecosystem, utilizing specialized libraries for hardware interaction and a high-performance training-to-deployment pipeline via GitHub and Hugging Face.\n",
    "\n",
    "| Component | Technology | Justification |\n",
    "| :--- | :--- | :--- |\n",
    "| **Core Emulator Lib** | PyBoy | Provides the interface for cycle-accurate state access and memory manipulation within the Python environment. |\n",
    "| **ML Framework** | PyTorch | Essential for dynamic computation graphs required by Slot Attention and Transformers. Supports high-performance training with CUDA/AMP. |\n",
    "| **Data Processing** | NumPy & Custom Libraries | Optimized handling of large-scale binary snapshots and ND-arrays for VRAM-to-pixel conversion. |\n",
    "| **Experiment Tracking** | Weights & Biases (W&B) | I have not set this up yet. Tracks reconstruction loss, visual tile accuracy, and memory deltas across long training runs. |\n",
    "| **Model Registry** | Hugging Face Model Hub | Provides a centralized, versioned repository for the pre-trained CNN weights and the final Full-Stack NERV model. |\n",
    "| **Version Control** | Git + GitHub | Used for collaborative development, CI/CD for the custom library components, and automated code review. |\n",
    "| **Deployment Platform** | Hugging Face Spaces (Gradio) | Allows for an interactive, web-based demonstration of the model \"playing\" or hallucinating game states in real-time. |\n",
    "| **State Acquisition** | Custom Python Tooling | Specialized libraries used to extract and decode VRAM/WRAM snapshots from JSON-based execution recordings. |\n",
    "| **Documentation** | Jupyter Notebooks| Combines live code execution with architectural explanations for the multi-stage training pipeline. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppO8XXSg32Jn"
   },
   "source": [
    "---\n",
    "\n",
    "## Section 7: Evaluation Strategy\n",
    "\n",
    "### 7.1 Success Metrics\n",
    "\n",
    "Define **quantitative metrics** for evaluating your project:\n",
    "\n",
    "| Metric | Target Value | Measurement Method | Priority |\n",
    "|--------|--------------|-------------------|----------|\n",
    "| <span style=\"color:blue\">[Metric 1]</span> | <span style=\"color:blue\">[Target]</span> | <span style=\"color:blue\">[Method]</span> | <span style=\"color:blue\">[Must-have/Should-have/Nice-to-have]</span> |\n",
    "| <span style=\"color:blue\">[Metric 2]</span> | <span style=\"color:blue\">[Target]</span> | <span style=\"color:blue\">[Method]</span> | <span style=\"color:blue\">[Priority]</span> |\n",
    "| <span style=\"color:blue\">[Metric 3]</span> | <span style=\"color:blue\">[Target]</span> | <span style=\"color:blue\">[Method]</span> | <span style=\"color:blue\">[Priority]</span> |\n",
    "\n",
    "**Example Metrics:**\n",
    "| Metric | Target Value | Measurement Method | Priority |\n",
    "|--------|--------------|-------------------|----------|\n",
    "| Classification Accuracy | >85% | Test set evaluation | Must-have |\n",
    "| F1 Score (macro) | >0.80 | Test set evaluation | Must-have |\n",
    "| Inference Latency | <500ms | End-to-end timing | Should-have |\n",
    "| Model Size | <100MB | File size measurement | Nice-to-have |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe78JVYY32Jn"
   },
   "source": [
    "### 7.2 Evaluation Methodology\n",
    "\n",
    "**Train/Validation/Test Split:**\n",
    "\n",
    "<span style=\"color:blue\">[Describe your data splitting strategy and rationale. Example: 70% train, 15% validation, 15% test, stratified by class]</span>\n",
    "\n",
    "**Cross-Validation Plan:**\n",
    "\n",
    "<span style=\"color:blue\">[Will you use cross-validation? What kind and why?]</span>\n",
    "\n",
    "**Evaluation Frequency:**\n",
    "\n",
    "<span style=\"color:blue\">[How often will you evaluate during training? What triggers a checkpoint?]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRUNAHA432Jo"
   },
   "source": [
    "### 7.3 Baseline Comparisons\n",
    "\n",
    "You must compare your model against baselines. List your planned comparisons:\n",
    "\n",
    "| Baseline | Expected Performance | Purpose |\n",
    "|----------|---------------------|--------|\n",
    "| Random/Majority Class | <span style=\"color:blue\">[Expected]</span> | Shows model is learning |\n",
    "| Simple Model (e.g., Logistic Regression) | <span style=\"color:blue\">[Expected]</span> | Justifies complexity |\n",
    "| Existing Solution (if any) | <span style=\"color:blue\">[Expected]</span> | Shows improvement |\n",
    "| Human Performance (if measurable) | <span style=\"color:blue\">[Expected]</span> | Establishes ceiling |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLQ4ByIs32Jo"
   },
   "source": [
    "### 7.4 Qualitative Evaluation\n",
    "\n",
    "How will you evaluate aspects that can't be measured numerically?\n",
    "\n",
    "<span style=\"color:blue\">[Describe your plan for user testing, expert review, or other qualitative evaluation]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCxyyRDl32Jo"
   },
   "source": [
    "---\n",
    "\n",
    "## Section 8: Deployment Strategy\n",
    "\n",
    "At a high level, please detail the way in which you plan to deploy your project. You don't need to know the details of how to do this yet, but its important to think about the end state of how you would have users interact with your system once its completed. This can affect many other components of your project including your requirements, system architecture, and timeline. For example:\n",
    "\n",
    "**Deployment Target:** <span style=\"color:blue\">[REST API / Web Application / Mobile App / Desktop App / Batch Pipeline / Serverless / Other]</span>\n",
    "\n",
    "**Deployment Platform:**\n",
    "\n",
    "<span style=\"color:blue\">[Where will you deploy? Local, Managed Cloud Platofrms, Hosted Inference Services, Serverless, High-Performance Inference Serves, Simple App Frameworks (e.g. FastAPI)]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DnJCcFN32Jp"
   },
   "source": [
    "---\n",
    "\n",
    "## Section 9: Timeline & Milestones\n",
    "\n",
    "### 9.1 Project Phases\n",
    "\n",
    "Break your project into phases aligned with the semester schedule:\n",
    "\n",
    "| Phase | Week(s) | Deliverables | Checkpoint |\n",
    "|-------|---------|--------------|------------|\n",
    "| 1: Setup & Data | 1-2 | <span style=\"color:blue\">[Deliverables]</span> | <span style=\"color:blue\">[Checkpoint]</span> |\n",
    "| 2: Baseline & EDA | 3-4 | <span style=\"color:blue\">[Deliverables]</span> | <span style=\"color:blue\">[Checkpoint]</span> |\n",
    "| 3: Model Development | 5-7 | <span style=\"color:blue\">[Deliverables]</span> | <span style=\"color:blue\">[Checkpoint]</span> |\n",
    "| 4: Iteration & Improvement | 8-10 | <span style=\"color:blue\">[Deliverables]</span> | <span style=\"color:blue\">[Checkpoint]</span> |\n",
    "| 5: Deployment & Polish | 11-13 | <span style=\"color:blue\">[Deliverables]</span> | <span style=\"color:blue\">[Checkpoint]</span> |\n",
    "| 6: Documentation & Presentation | 14-15 | <span style=\"color:blue\">[Deliverables]</span> | <span style=\"color:blue\">[Checkpoint]</span> |\n",
    "\n",
    "**Example Phase Breakdown:**\n",
    "\n",
    "| Phase | Week(s) | Deliverables | Checkpoint |\n",
    "|-------|---------|--------------|------------|\n",
    "| 1: Setup & Data | 1-2 | Repo setup, data acquired, EDA notebook | Data Review |\n",
    "| 2: Baseline & EDA | 3-4 | Baseline model, data pipeline, initial metrics | Baseline Review |\n",
    "| 3: Model Development | 5-7 | Primary model trained, experiment tracking in place | Midterm Review |\n",
    "| 4: Iteration & Improvement | 8-10 | Model improvements, hyperparameter tuning complete | Progress Check |\n",
    "| 5: Deployment & Polish | 11-13 | API deployed, monitoring set up, tests passing | Deployment Review |\n",
    "| 6: Documentation & Presentation | 14-15 | Final report, presentation, code cleanup | Final Submission |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKnlVBoS32Jp"
   },
   "source": [
    "### 9.2 Weekly Goals\n",
    "\n",
    "For the first four weeks, define specific goals:\n",
    "\n",
    "**Week 1:**\n",
    "- [ ] <span style=\"color:blue\">[Goal 1]</span>\n",
    "- [ ] <span style=\"color:blue\">[Goal 2]</span>\n",
    "- [ ] <span style=\"color:blue\">[Goal 3]</span>\n",
    "\n",
    "**Week 2:**\n",
    "- [ ] <span style=\"color:blue\">[Goal 1]</span>\n",
    "- [ ] <span style=\"color:blue\">[Goal 2]</span>\n",
    "- [ ] <span style=\"color:blue\">[Goal 3]</span>\n",
    "\n",
    "**Week 3:**\n",
    "- [ ] <span style=\"color:blue\">[Goal 1]</span>\n",
    "- [ ] <span style=\"color:blue\">[Goal 2]</span>\n",
    "- [ ] <span style=\"color:blue\">[Goal 3]</span>\n",
    "\n",
    "**Week 4:**\n",
    "- [ ] <span style=\"color:blue\">[Goal 1]</span>\n",
    "- [ ] <span style=\"color:blue\">[Goal 2]</span>\n",
    "- [ ] <span style=\"color:blue\">[Goal 3]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO22muxp32Jp"
   },
   "source": [
    "### 9.3 Checkpoint Submissions\n",
    "\n",
    "List what you will submit at each checkpoint that you have identified:\n",
    "\n",
    "| Checkpoint | Due Date | Deliverables |\n",
    "|------------|----------|-------------|\n",
    "| Project Proposal (this document) | <span style=\"color:blue\">[Date]</span> | This completed notebook |\n",
    "| Data & Baseline Review | <span style=\"color:blue\">[Date]</span> | <span style=\"color:blue\">[Deliverables]</span> |\n",
    "| Midterm Progress Report | <span style=\"color:blue\">[Date]</span> | <span style=\"color:blue\">[Deliverables]</span> |\n",
    "| Deployment Demo | <span style=\"color:blue\">[Date]</span> | <span style=\"color:blue\">[Deliverables]</span> |\n",
    "| Final Submission | <span style=\"color:blue\">[Date]</span> | <span style=\"color:blue\">[Deliverables]</span> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHh7Hbc732J6"
   },
   "source": [
    "---\n",
    "\n",
    "## Section 10: Risk Assessment\n",
    "\n",
    "### 10.1 Risk Identification\n",
    "\n",
    "Identify potential risks and their mitigation strategies:\n",
    "\n",
    "| Risk | Likelihood (L/M/H) | Impact (L/M/H) | Mitigation Strategy | Contingency Plan |\n",
    "|------|-------------------|----------------|--------------------|-----------------|\n",
    "| <span style=\"color:blue\">[Risk 1]</span> | <span style=\"color:blue\">[L/M/H]</span> | <span style=\"color:blue\">[L/M/H]</span> | <span style=\"color:blue\">[Mitigation]</span> | <span style=\"color:blue\">[Contingency]</span> |\n",
    "| <span style=\"color:blue\">[Risk 2]</span> | <span style=\"color:blue\">[L/M/H]</span> | <span style=\"color:blue\">[L/M/H]</span> | <span style=\"color:blue\">[Mitigation]</span> | <span style=\"color:blue\">[Contingency]</span> |\n",
    "| <span style=\"color:blue\">[Risk 3]</span> | <span style=\"color:blue\">[L/M/H]</span> | <span style=\"color:blue\">[L/M/H]</span> | <span style=\"color:blue\">[Mitigation]</span> | <span style=\"color:blue\">[Contingency]</span> |\n",
    "| <span style=\"color:blue\">[Risk 4]</span> | <span style=\"color:blue\">[L/M/H]</span> | <span style=\"color:blue\">[L/M/H]</span> | <span style=\"color:blue\">[Mitigation]</span> | <span style=\"color:blue\">[Contingency]</span> |\n",
    "| <span style=\"color:blue\">[Risk 5]</span> | <span style=\"color:blue\">[L/M/H]</span> | <span style=\"color:blue\">[L/M/H]</span> | <span style=\"color:blue\">[Mitigation]</span> | <span style=\"color:blue\">[Contingency]</span> |\n",
    "\n",
    "**Common Risks to Consider:**\n",
    "- Data unavailability or quality issues\n",
    "- Model underperformance\n",
    "- Computational resource limitations\n",
    "- Technical skill gaps\n",
    "- Time constraints / scope creep\n",
    "- Dependency/library issues\n",
    "- Deployment platform issues\n",
    "\n",
    "**Example Risk Entry:**\n",
    "| Risk | Likelihood | Impact | Mitigation | Contingency |\n",
    "|------|------------|--------|------------|-------------|\n",
    "| Primary dataset becomes unavailable | Low | High | Download and store locally immediately | Use PlantVillage dataset as backup |\n",
    "| Model accuracy below target | Medium | Medium | Start with proven architecture, iterate | Reduce scope to fewer disease classes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osl6rvpP32J6"
   },
   "source": [
    "### 10.2 Assumptions\n",
    "\n",
    "List assumptions you are making that, if wrong, could affect your project:\n",
    "\n",
    "1. <span style=\"color:blue\">[Assumption 1]</span>\n",
    "2. <span style=\"color:blue\">[Assumption 2]</span>\n",
    "3. <span style=\"color:blue\">[Assumption 3]</span>\n",
    "4. <span style=\"color:blue\">[Assumption 4]</span>\n",
    "5. <span style=\"color:blue\">[Assumption 5]</span>\n",
    "\n",
    "**Example Assumptions:**\n",
    "1. The public dataset labels are accurate and consistent\n",
    "2. I will have access to a GPU for training (university cluster or Google Colab)\n",
    "3. Users will have smartphones with decent cameras\n",
    "4. 10,000 images will be sufficient for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQdGOPEc32J7"
   },
   "source": [
    "### 10.3 Dependencies\n",
    "\n",
    "List external dependencies that could affect your timeline:\n",
    "\n",
    "| Dependency | Type | Criticality | Alternative |\n",
    "|------------|------|-------------|-------------|\n",
    "| <span style=\"color:blue\">[Dependency 1]</span> | <span style=\"color:blue\">[Type]</span> | <span style=\"color:blue\">[Low/Medium/High]</span> | <span style=\"color:blue\">[Alternative]</span> |\n",
    "| <span style=\"color:blue\">[Dependency 2]</span> | <span style=\"color:blue\">[Type]</span> | <span style=\"color:blue\">[Low/Medium/High]</span> | <span style=\"color:blue\">[Alternative]</span> |\n",
    "| <span style=\"color:blue\">[Dependency 3]</span> | <span style=\"color:blue\">[Type]</span> | <span style=\"color:blue\">[Low/Medium/High]</span> | <span style=\"color:blue\">[Alternative]</span> |\n",
    "\n",
    "**Example Dependencies:**\n",
    "| Dependency | Type | Criticality | Alternative |\n",
    "|------------|------|-------------|-------------|\n",
    "| AWS Free Tier | Platform | Medium | Google Cloud, local deployment |\n",
    "| Pre-trained EfficientNet weights | Model | High | Train from scratch (longer) |\n",
    "| Plant pathology expert for validation | Human | Low | Rely on dataset labels only |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqhRIQFU32J9"
   },
   "source": [
    "---\n",
    "\n",
    "## Appendix A: Project Complexity Guidelines\n",
    "\n",
    "Your project should fall within the \"Appropriate\" range:\n",
    "\n",
    "| Level | Characteristics | Example |\n",
    "|-------|-----------------|--------|\n",
    "| **Too Simple** | Tutorial-level, no novel combination, single notebook | Image classifier with dataset from Kaggle |\n",
    "| **Appropriate** | Real-world problem, multiple components, deployed | Plant disease detector with API, monitoring, and CI/CD, extensive evals |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
